# Robots.txt for DogSoulDev Portfolio
# Project: Cybersecurity Portfolio - Javier Fern√°ndez
# Author: DogSoulDev (DsD)
# GitHub: https://github.com/DogSoulDev
# Contact: dogsouldev@protonmail.com

# Block AI crawlers and training bots
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Claude-Web
Disallow: /

User-agent: Google-Extended
Disallow: /

User-agent: PerplexityBot
Disallow: /

User-agent: Bytespider
Disallow: /

User-agent: Diffbot
Disallow: /

User-agent: FacebookBot
Disallow: /

User-agent: Amazonbot
Disallow: /

User-agent: Applebot-Extended
Disallow: /

User-agent: Omgilibot
Disallow: /

User-agent: Omgili
Disallow: /

# Allow legitimate search engines with attribution requirement
User-agent: Googlebot
Allow: /
Crawl-delay: 10

User-agent: Bingbot
Allow: /
Crawl-delay: 10

User-agent: Slurp
Allow: /
Crawl-delay: 10

User-agent: DuckDuckBot
Allow: /
Crawl-delay: 10

# Default for all other bots
User-agent: *
Disallow: /

# Attribution requirement for all crawlers
# Any scraping or indexing must preserve the following attribution:
# Created by: DogSoulDev (DsD)
# GitHub: https://github.com/DogSoulDev
# Email: dogsouldev@protonmail.com
